{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from char_rnn import *\n",
    "import shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n",
      "First Citi\n"
     ]
    }
   ],
   "source": [
    "with open(\"tiny_shakespeare.txt\", 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(len(text))\n",
    "text = text[:1000]\n",
    "print(text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAl')\n",
      "(100, 'articularise their abundance; our\\nsufferance is a gain to them L')\n",
      "(100, 're we become rakes: for the gods know I\\nspeak this in hunger for')\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "train_size = int(0.8 * len(text))\n",
    "valid_size = int(0.1 * len(text))\n",
    "test_size = len(text) - train_size - valid_size\n",
    "train_text = text[:train_size]\n",
    "valid_text = text[train_size:train_size + valid_size]\n",
    "test_text = text[train_size + valid_size:]\n",
    "\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])\n",
    "print(test_size, test_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 46\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary\n",
    "unique_chars = list(set(text))\n",
    "vocab_size = len(unique_chars)\n",
    "print('vocab size: %d' % vocab_size)\n",
    "vocab_index_dict = {}\n",
    "index_vocab_dict = {}\n",
    "\n",
    "for i, char in enumerate(unique_chars):\n",
    "    vocab_index_dict[char] = i\n",
    "    index_vocab_dict[i] = char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create batch generators\n",
    "batch_size = 10\n",
    "n_unrollings = 10\n",
    "train_batches = BatchGenerator(train_text, batch_size, n_unrollings, vocab_size, \n",
    "                               vocab_index_dict, index_vocab_dict)\n",
    "eval_train_batches = BatchGenerator(train_text, 1, 1, vocab_size,\n",
    "                                    vocab_index_dict, index_vocab_dict)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1, vocab_size,\n",
    "                               vocab_index_dict, index_vocab_dict)\n",
    "test_batches = BatchGenerator(test_text, 1, 1, vocab_size,\n",
    "                              vocab_index_dict, index_vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citiz', '\\n\\nFirst Cit', 'ed. resolve', 'e people.\\n\\n', ' have corn ', 'be done: aw', 'e are accou', 'ould reliev', 'some, we mi', 'the leannes']\n",
      "['ar']\n"
     ]
    }
   ],
   "source": [
    "# Test batch generators\n",
    "print(batches2string(train_batches.next(), index_vocab_dict))\n",
    "print(batches2string(valid_batches.next(), index_vocab_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare parameters\n",
    "params = {'batch_size': batch_size, 'num_unrollings': n_unrollings, 'vocab_size': vocab_size, \n",
    "        'hidden_size': 100, 'max_grad_norm': 5.0, 'embedding_size': 50, \n",
    "        'num_layers': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create graphs\n",
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.variable_scope('char_rnn') as scope:\n",
    "        with tf.name_scope('training'):\n",
    "            train_model = CharRNN(is_training=True, **params)\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        with tf.name_scope('evaluation'):\n",
    "            valid_model = CharRNN(is_training=False, **params)\n",
    "        # test_model = CharRNN(is_training=False, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_epochs = 10\n",
    "model_name = 'char_rnn'\n",
    "saved_model_dir = '/tmp/char_rnn/char_rnn'\n",
    "init_from_path = '' #'/tmp/saved_char_rnn_model-240'\n",
    "# init_from_path = saved_path\n",
    "log_dir = '/tmp/charRNNTrainLog1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size (number of parameters): 67346\n",
      "\n",
      "Epoch 0\n",
      "\n",
      "training\n",
      "epoch_size: 7\n",
      "data_size: 800\n",
      "num_unrollings: 10\n",
      "batch_size: 10\n",
      "final ppl: 45.650, speed: 3121 wps\n",
      "model saved in /tmp/char_rnn/char_rnn-7\n",
      "\n",
      "validation\n",
      "epoch_size: 99\n",
      "data_size: 100\n",
      "num_unrollings: 1\n",
      "batch_size: 1\n",
      "final ppl: 34.350, speed: 1233 wps\n",
      "\n",
      "Epoch 1\n",
      "\n",
      "training\n",
      "epoch_size: 7\n",
      "data_size: 800\n",
      "num_unrollings: 10\n",
      "batch_size: 10\n",
      "final ppl: 34.230, speed: 4176 wps\n",
      "model saved in /tmp/char_rnn/char_rnn-14\n",
      "\n",
      "validation\n",
      "epoch_size: 99\n",
      "data_size: 100\n",
      "num_unrollings: 1\n",
      "batch_size: 1\n",
      "final ppl: 26.405, speed: 946 wps\n",
      "\n",
      "Epoch 2\n",
      "\n",
      "training\n",
      "epoch_size: 7\n",
      "data_size: 800\n",
      "num_unrollings: 10\n",
      "batch_size: 10\n",
      "final ppl: 28.142, speed: 3538 wps\n",
      "model saved in /tmp/char_rnn/char_rnn-21\n",
      "\n",
      "validation\n",
      "epoch_size: 99\n",
      "data_size: 100\n",
      "num_unrollings: 1\n",
      "batch_size: 1\n",
      "final ppl: 22.686, speed: 1037 wps\n",
      "\n",
      "Epoch 3\n",
      "\n",
      "training\n",
      "epoch_size: 7\n",
      "data_size: 800\n",
      "num_unrollings: 10\n",
      "batch_size: 10\n",
      "final ppl: 25.551, speed: 3590 wps\n",
      "model saved in /tmp/char_rnn/char_rnn-28\n",
      "\n",
      "validation\n",
      "epoch_size: 99\n",
      "data_size: 100\n",
      "num_unrollings: 1\n",
      "batch_size: 1\n",
      "final ppl: 21.864, speed: 1118 wps\n",
      "\n",
      "Epoch 4\n",
      "\n",
      "training\n",
      "epoch_size: 7\n",
      "data_size: 800\n",
      "num_unrollings: 10\n",
      "batch_size: 10\n",
      "final ppl: 24.911, speed: 3709 wps\n",
      "model saved in /tmp/char_rnn/char_rnn-35\n",
      "\n",
      "validation\n",
      "epoch_size: 99\n",
      "data_size: 100\n",
      "num_unrollings: 1\n",
      "batch_size: 1\n",
      "final ppl: 21.582, speed: 1156 wps\n",
      "\n",
      "Epoch 5\n",
      "\n",
      "training\n",
      "epoch_size: 7\n",
      "data_size: 800\n",
      "num_unrollings: 10\n",
      "batch_size: 10\n",
      "final ppl: 24.224, speed: 3900 wps\n",
      "model saved in /tmp/char_rnn/char_rnn-42\n",
      "\n",
      "validation\n",
      "epoch_size: 99\n",
      "data_size: 100\n",
      "num_unrollings: 1\n",
      "batch_size: 1\n",
      "final ppl: 21.179, speed: 1150 wps\n",
      "\n",
      "Epoch 6\n",
      "\n",
      "training\n",
      "epoch_size: 7\n",
      "data_size: 800\n",
      "num_unrollings: 10\n",
      "batch_size: 10\n",
      "final ppl: 24.529, speed: 4128 wps\n",
      "model saved in /tmp/char_rnn/char_rnn-49\n",
      "\n",
      "validation\n",
      "epoch_size: 99\n",
      "data_size: 100\n",
      "num_unrollings: 1\n",
      "batch_size: 1\n",
      "final ppl: 21.464, speed: 1145 wps\n",
      "\n",
      "Epoch 7\n",
      "\n",
      "training\n",
      "epoch_size: 7\n",
      "data_size: 800\n",
      "num_unrollings: 10\n",
      "batch_size: 10\n",
      "final ppl: 23.888, speed: 4071 wps\n",
      "model saved in /tmp/char_rnn/char_rnn-56\n",
      "\n",
      "validation\n",
      "epoch_size: 99\n",
      "data_size: 100\n",
      "num_unrollings: 1\n",
      "batch_size: 1\n",
      "final ppl: 20.850, speed: 1235 wps\n",
      "\n",
      "Epoch 8\n",
      "\n",
      "training\n",
      "epoch_size: 7\n",
      "data_size: 800\n",
      "num_unrollings: 10\n",
      "batch_size: 10\n",
      "final ppl: 23.032, speed: 4052 wps\n",
      "model saved in /tmp/char_rnn/char_rnn-63\n",
      "\n",
      "validation\n",
      "epoch_size: 99\n",
      "data_size: 100\n",
      "num_unrollings: 1\n",
      "batch_size: 1\n",
      "final ppl: 20.119, speed: 971 wps\n",
      "\n",
      "Epoch 9\n",
      "\n",
      "training\n",
      "epoch_size: 7\n",
      "data_size: 800\n",
      "num_unrollings: 10\n",
      "batch_size: 10\n",
      "final ppl: 23.199, speed: 3509 wps\n",
      "model saved in /tmp/char_rnn/char_rnn-70\n",
      "\n",
      "validation\n",
      "epoch_size: 99\n",
      "data_size: 100\n",
      "num_unrollings: 1\n",
      "batch_size: 1\n",
      "final ppl: 20.738, speed: 1119 wps\n",
      "\n",
      "test\n",
      "epoch_size: 99\n",
      "data_size: 100\n",
      "num_unrollings: 1\n",
      "batch_size: 1\n",
      "final ppl: 20.780, speed: 1010 wps\n"
     ]
    }
   ],
   "source": [
    "shutil.rmtree(log_dir)\n",
    "print('model size (number of parameters): %s' % train_model.model_size)\n",
    "with tf.Session(graph=graph) as session:\n",
    "    writer = tf.train.SummaryWriter(log_dir, session.graph_def)\n",
    "    if init_from_path:\n",
    "        train_model.saver.restore(session, init_from_path)\n",
    "    else:\n",
    "        tf.initialize_all_variables().run()\n",
    "        \n",
    "    for i in range(n_epochs):\n",
    "        print('\\nEpoch %d\\n' % i)\n",
    "        print('training')\n",
    "        ppl, train_summary_str, global_step = train_model.run_epoch(session, train_size, train_batches, \n",
    "                                                 is_training=True, verbose=False)\n",
    "\n",
    "        writer.add_summary(train_summary_str, global_step)\n",
    "        saved_path = train_model.saver.save(session, saved_model_dir, \n",
    "                                            global_step=train_model.global_step)\n",
    "\n",
    "        print('model saved in %s\\n' % saved_path)\n",
    "        print('validation')\n",
    "        valid_ppl, valid_summary_str, _ = valid_model.run_epoch(session, valid_size, valid_batches, \n",
    "                                                         is_training=False, verbose=False)\n",
    "        writer.add_summary(valid_summary_str, global_step)\n",
    "        writer.flush()\n",
    "    print('\\ntest')\n",
    "    valid_model.run_epoch(session, test_size, test_batches, \n",
    "                         is_training=False, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fire  iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit i\n"
     ]
    }
   ],
   "source": [
    "# Sampling some sequence that start\n",
    "start_text = 'Fir'\n",
    "with tf.Session(graph=graph) as session:\n",
    "    train_model.saver.restore(session, saved_path)\n",
    "    print(valid_model.sample_seq(session, 500, start_text, \n",
    "                                 vocab_index_dict, index_vocab_dict, max_prob=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the loss of some sequence. \n",
    "example_text = 'First sissssssssssss'\n",
    "# example_text = train_text[:20]\n",
    "example_batches = BatchGenerator(example_text, 1, 1, vocab_size, vocab_index_dict, index_vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_size: 19\n",
      "data_size: 20\n",
      "num_unrollings: 1\n",
      "batch_size: 1\n",
      "47.4%, step:9, perplexity: 19.067, speed: 454 wps\n",
      "final ppl: 24.814, speed: 689 wps\n",
      "24.8144\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    train_model.saver.restore(sess, saved_path)\n",
    "    print(valid_model.run_epoch(sess, len(try_text), try_batches,\n",
    "                        is_training=False, verbose=True, debug=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ef1 = '/tmp/char_rnn_logs_6/events.out'\n",
    "# ef2 = '/tmp/char_rnn_logs_3/events.out.tfevents.1457078736.Chens-MacBook-Pro.local'\n",
    "# for e in tf.train.summary_iterator(ef1):\n",
    "#     for v in e.summary.value:\n",
    "#         if v.tag == 'training perplexity':\n",
    "#             print(v.simple_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
