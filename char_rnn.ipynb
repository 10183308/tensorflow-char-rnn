{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from char_rnn import *\n",
    "import shutil, os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1115394\n",
      "First Citi\n"
     ]
    }
   ],
   "source": [
    "with open(\"data/tiny_shakespeare.txt\", 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print(len(text))\n",
    "text = text[:1000]\n",
    "print(text[:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(800, 'First Citizen:\\nBefore we proceed any further, hear me speak.\\n\\nAl')\n",
      "(100, 'articularise their abundance; our\\nsufferance is a gain to them L')\n",
      "(100, 're we become rakes: for the gods know I\\nspeak this in hunger for')\n"
     ]
    }
   ],
   "source": [
    "# prepare data\n",
    "train_size = int(0.8 * len(text))\n",
    "valid_size = int(0.1 * len(text))\n",
    "test_size = len(text) - train_size - valid_size\n",
    "train_text = text[:train_size]\n",
    "valid_text = text[train_size:train_size + valid_size]\n",
    "test_text = text[train_size + valid_size:]\n",
    "\n",
    "print(train_size, train_text[:64])\n",
    "print(valid_size, valid_text[:64])\n",
    "print(test_size, test_text[:64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vocab size: 46\n"
     ]
    }
   ],
   "source": [
    "# Create vocabulary\n",
    "unique_chars = list(set(text))\n",
    "vocab_size = len(unique_chars)\n",
    "print('vocab size: %d' % vocab_size)\n",
    "vocab_index_dict = {}\n",
    "index_vocab_dict = {}\n",
    "\n",
    "for i, char in enumerate(unique_chars):\n",
    "    vocab_index_dict[char] = i\n",
    "    index_vocab_dict[i] = char"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create batch generators\n",
    "batch_size = 10\n",
    "n_unrollings = 10\n",
    "train_batches = BatchGenerator(train_text, batch_size, n_unrollings, vocab_size, \n",
    "                               vocab_index_dict, index_vocab_dict)\n",
    "eval_train_batches = BatchGenerator(train_text, 1, 1, vocab_size,\n",
    "                                    vocab_index_dict, index_vocab_dict)\n",
    "valid_batches = BatchGenerator(valid_text, 1, 1, vocab_size,\n",
    "                               vocab_index_dict, index_vocab_dict)\n",
    "test_batches = BatchGenerator(test_text, 1, 1, vocab_size,\n",
    "                              vocab_index_dict, index_vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['First Citiz', '\\n\\nFirst Cit', 'ed. resolve', 'e people.\\n\\n', ' have corn ', 'be done: aw', 'e are accou', 'ould reliev', 'some, we mi', 'the leannes']\n",
      "['ar']\n"
     ]
    }
   ],
   "source": [
    "# Test batch generators\n",
    "print(batches2string(train_batches.next(), index_vocab_dict))\n",
    "print(batches2string(valid_batches.next(), index_vocab_dict))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Prepare parameters\n",
    "params = {'batch_size': batch_size, 'num_unrollings': n_unrollings, 'vocab_size': vocab_size, \n",
    "        'hidden_size': 100, 'max_grad_norm': 5.0, 'embedding_size': 50, \n",
    "        'num_layers': 1}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Create graphs\n",
    "tf.reset_default_graph()\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "    with tf.variable_scope('char_rnn') as scope:\n",
    "        with tf.name_scope('training'):\n",
    "            train_model = CharRNN(is_training=True, **params)\n",
    "        tf.get_variable_scope().reuse_variables()\n",
    "        with tf.name_scope('evaluation'):\n",
    "            valid_model = CharRNN(is_training=False, **params)\n",
    "        # test_model = CharRNN(is_training=False, **params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "n_epochs = 10\n",
    "model_name = 'char_rnn'\n",
    "saved_model_dir = '/tmp/char_rnn/char_rnn'\n",
    "init_from_path = '' #'/tmp/saved_char_rnn_model-240'\n",
    "# init_from_path = saved_path\n",
    "log_dir = '/tmp/charRNNTrainLog1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "if os.path.exists(log_dir):\n",
    "    shutil.rmtree(log_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model size (number of parameters): 67346\n",
      "\n",
      "Epoch 0\n",
      "\n",
      "training\n",
      "epoch_size: 7\n",
      "data_size: 800\n",
      "num_unrollings: 10\n",
      "batch_size: 10\n",
      "final ppl: 67.370, speed: 3254 wps\n",
      "model saved in /tmp/char_rnn/char_rnn-7\n",
      "\n",
      "validation\n",
      "epoch_size: 99\n",
      "data_size: 100\n",
      "num_unrollings: 1\n",
      "batch_size: 1\n",
      "final ppl: 50.066, speed: 1222 wps\n",
      "\n",
      "Epoch 1\n",
      "\n",
      "training\n",
      "epoch_size: 7\n",
      "data_size: 800\n",
      "num_unrollings: 10\n",
      "batch_size: 10\n",
      "final ppl: 39.808, speed: 4086 wps\n",
      "model saved in /tmp/char_rnn/char_rnn-14\n",
      "\n",
      "validation\n",
      "epoch_size: 99\n",
      "data_size: 100\n",
      "num_unrollings: 1\n",
      "batch_size: 1\n",
      "final ppl: 27.388, speed: 1060 wps\n",
      "\n",
      "Epoch 2\n",
      "\n",
      "training\n",
      "epoch_size: 7\n",
      "data_size: 800\n",
      "num_unrollings: 10\n",
      "batch_size: 10\n",
      "final ppl: 27.128, speed: 2942 wps\n",
      "model saved in /tmp/char_rnn/char_rnn-21\n",
      "\n",
      "validation\n",
      "epoch_size: 99\n",
      "data_size: 100\n",
      "num_unrollings: 1\n",
      "batch_size: 1\n",
      "final ppl: 22.532, speed: 908 wps\n",
      "\n",
      "Epoch 3\n",
      "\n",
      "training\n",
      "epoch_size: 7\n",
      "data_size: 800\n",
      "num_unrollings: 10\n",
      "batch_size: 10\n",
      "final ppl: 25.860, speed: 3578 wps\n",
      "model saved in /tmp/char_rnn/char_rnn-28\n",
      "\n",
      "validation\n",
      "epoch_size: 99\n",
      "data_size: 100\n",
      "num_unrollings: 1\n",
      "batch_size: 1\n",
      "final ppl: 22.781, speed: 1134 wps\n",
      "\n",
      "Epoch 4\n",
      "\n",
      "training\n",
      "epoch_size: 7\n",
      "data_size: 800\n",
      "num_unrollings: 10\n",
      "batch_size: 10\n",
      "final ppl: 25.425, speed: 3964 wps\n",
      "model saved in /tmp/char_rnn/char_rnn-35\n",
      "\n",
      "validation\n",
      "epoch_size: 99\n",
      "data_size: 100\n",
      "num_unrollings: 1\n",
      "batch_size: 1\n",
      "final ppl: 22.180, speed: 952 wps\n",
      "\n",
      "Epoch 5\n",
      "\n",
      "training\n",
      "epoch_size: 7\n",
      "data_size: 800\n",
      "num_unrollings: 10\n",
      "batch_size: 10\n",
      "final ppl: 24.934, speed: 3116 wps\n",
      "model saved in /tmp/char_rnn/char_rnn-42\n",
      "\n",
      "validation\n",
      "epoch_size: 99\n",
      "data_size: 100\n",
      "num_unrollings: 1\n",
      "batch_size: 1\n",
      "final ppl: 22.311, speed: 991 wps\n",
      "\n",
      "Epoch 6\n",
      "\n",
      "training\n",
      "epoch_size: 7\n",
      "data_size: 800\n",
      "num_unrollings: 10\n",
      "batch_size: 10\n",
      "final ppl: 24.624, speed: 3626 wps\n",
      "model saved in /tmp/char_rnn/char_rnn-49\n",
      "\n",
      "validation\n",
      "epoch_size: 99\n",
      "data_size: 100\n",
      "num_unrollings: 1\n",
      "batch_size: 1\n",
      "final ppl: 22.273, speed: 1112 wps\n",
      "\n",
      "Epoch 7\n",
      "\n",
      "training\n",
      "epoch_size: 7\n",
      "data_size: 800\n",
      "num_unrollings: 10\n",
      "batch_size: 10\n",
      "final ppl: 23.858, speed: 4147 wps\n",
      "model saved in /tmp/char_rnn/char_rnn-56\n",
      "\n",
      "validation\n",
      "epoch_size: 99\n",
      "data_size: 100\n",
      "num_unrollings: 1\n",
      "batch_size: 1\n",
      "final ppl: 21.430, speed: 1210 wps\n",
      "\n",
      "Epoch 8\n",
      "\n",
      "training\n",
      "epoch_size: 7\n",
      "data_size: 800\n",
      "num_unrollings: 10\n",
      "batch_size: 10\n",
      "final ppl: 24.316, speed: 4142 wps\n",
      "model saved in /tmp/char_rnn/char_rnn-63\n",
      "\n",
      "validation\n",
      "epoch_size: 99\n",
      "data_size: 100\n",
      "num_unrollings: 1\n",
      "batch_size: 1\n",
      "final ppl: 21.193, speed: 1227 wps\n",
      "\n",
      "Epoch 9\n",
      "\n",
      "training\n",
      "epoch_size: 7\n",
      "data_size: 800\n",
      "num_unrollings: 10\n",
      "batch_size: 10\n",
      "final ppl: 23.532, speed: 4210 wps\n",
      "model saved in /tmp/char_rnn/char_rnn-70\n",
      "\n",
      "validation\n",
      "epoch_size: 99\n",
      "data_size: 100\n",
      "num_unrollings: 1\n",
      "batch_size: 1\n",
      "final ppl: 20.008, speed: 1216 wps\n",
      "\n",
      "test\n",
      "epoch_size: 99\n",
      "data_size: 100\n",
      "num_unrollings: 1\n",
      "batch_size: 1\n",
      "final ppl: 21.439, speed: 1227 wps\n"
     ]
    }
   ],
   "source": [
    "print('model size (number of parameters): %s' % train_model.model_size)\n",
    "with tf.Session(graph=graph) as session:\n",
    "    writer = tf.train.SummaryWriter(log_dir, session.graph_def)\n",
    "    if init_from_path:\n",
    "        train_model.saver.restore(session, init_from_path)\n",
    "    else:\n",
    "        tf.initialize_all_variables().run()\n",
    "        \n",
    "    for i in range(n_epochs):\n",
    "        print('\\nEpoch %d\\n' % i)\n",
    "        print('training')\n",
    "        ppl, train_summary_str, global_step = train_model.run_epoch(session, train_size, train_batches, \n",
    "                                                 is_training=True, verbose=False)\n",
    "\n",
    "        writer.add_summary(train_summary_str, global_step)\n",
    "        saved_path = train_model.saver.save(session, saved_model_dir, \n",
    "                                            global_step=train_model.global_step)\n",
    "\n",
    "        print('model saved in %s\\n' % saved_path)\n",
    "        print('validation')\n",
    "        valid_ppl, valid_summary_str, _ = valid_model.run_epoch(session, valid_size, valid_batches, \n",
    "                                                         is_training=False, verbose=False)\n",
    "        writer.add_summary(valid_summary_str, global_step)\n",
    "        writer.flush()\n",
    "    print('\\ntest')\n",
    "    valid_model.run_epoch(session, test_size, test_batches, \n",
    "                         is_training=False, verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fire  iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit iit i\n"
     ]
    }
   ],
   "source": [
    "# Sampling some sequence that start\n",
    "start_text = 'Fir'\n",
    "with tf.Session(graph=graph) as session:\n",
    "    train_model.saver.restore(session, saved_path)\n",
    "    print(valid_model.sample_seq(session, 500, start_text, \n",
    "                                 vocab_index_dict, index_vocab_dict, max_prob=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Compute the loss of some sequence. \n",
    "example_text = 'First sissssssssssss'\n",
    "# example_text = train_text[:20]\n",
    "example_batches = BatchGenerator(example_text, 1, 1, vocab_size, vocab_index_dict, index_vocab_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch_size: 19\n",
      "data_size: 20\n",
      "num_unrollings: 1\n",
      "batch_size: 1\n",
      "47.4%, step:9, perplexity: 19.067, speed: 454 wps\n",
      "final ppl: 24.814, speed: 689 wps\n",
      "24.8144\n"
     ]
    }
   ],
   "source": [
    "with tf.Session(graph=graph) as sess:\n",
    "    train_model.saver.restore(sess, saved_path)\n",
    "    print(valid_model.run_epoch(sess, len(try_text), try_batches,\n",
    "                        is_training=False, verbose=True, debug=False)[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# ef1 = '/tmp/char_rnn_logs_6/events.out'\n",
    "# ef2 = '/tmp/char_rnn_logs_3/events.out.tfevents.1457078736.Chens-MacBook-Pro.local'\n",
    "# for e in tf.train.summary_iterator(ef1):\n",
    "#     for v in e.summary.value:\n",
    "#         if v.tag == 'training perplexity':\n",
    "#             print(v.simple_value)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
